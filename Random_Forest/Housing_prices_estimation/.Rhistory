library(MASS)
data(package="MASS")
Boston
boston<-Boston
dim(boston)
names(boston)
#-------------- Pregunta 2: datos de train ----------#
set.seed(101)
train = sample(1:nrow(boston), 300)
head(boston)
#-------------- Pregunta 3: Ajuste del modelo ----------#
rf.boston = randomForest(medv~, data = boston, subset = train)
#-------------- Pregunta 3: Ajuste del modelo ----------#
rf.boston = randomForest(medv~., data = boston, subset = train)
rf.boston
plot(rf.boston)
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
oob.err[mtry] = fit$mse[350]
pred = predict(fit, boston[-train,])
test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
plot(test.err)
plot(rf.boston)
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
oob.err[mtry] = fit$mse[350]
pred = predict(fit, boston[-train,])
test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
plot(test.err)
?randomForest
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
#Borramos los datos
rm(list=ls())
library(MASS)
data(package="MASS")
boston<-Boston
dim(boston)
names(boston)
#-------------- Pregunta 2: datos de train ----------#
set.seed(101)
train = sample(1:nrow(boston), 300)
#-------------- Pregunta 3: Ajuste del modelo ----------#
rf.boston = randomForest(medv~., data = boston, subset = train)
rf.boston
plot(rf.boston)
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
oob.err[mtry] = fit$mse[350]
pred = predict(fit, boston[-train,])
test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
boost.boston = gbm(medv~., data = boston[train,], distribution = "gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)
summary(boost.boston)
plot(boost.boston,i="lstat")
plot(boost.boston,i="rm")
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = boston[-train,], n.trees = n.trees)
dim(predmat)
boost.err = with(boston[-train,], apply( (predmat - medv)^2, 2, mean) )
plot(n.trees, boost.err, pch = 23, ylab = "Mean Squared Error", xlab = "# Trees", main = "Boosting Test Error")
abline(h = min(test.err), col = "red")
#Borramos los datos
rm(list=ls())
library(MASS)
boston<-Boston
dim(boston)
names(boston)
#-------------- Pregunta 2: datos de train ----------#
set.seed(101)
train = sample(1:nrow(boston), 300)
#-------------- Pregunta 3: Ajuste del modelo ----------#
rf.boston = randomForest(medv~., data = boston, subset = train)
rf.boston
plot(rf.boston)
oob.err = double(13)
test.err = double(13)
randomForest
?randomForest
nrow(bosto)
nrow(boston)
#-------------- Pregunta 2: datos de train ----------#
set.seed(101)
train = sample(1:nrow(boston), 300)
train = sample(1:nrow(boston), 300) #seleccionamos 300 valores para entrenar
#-------------- Pregunta 3: Ajuste del modelo ----------#
rf.boston = randomForest(medv~., data = boston, subset = train)
rf.boston
300/500
plot(rf.boston)
oob.err = double(13)
test.err = double(13)
mtry
mtry=1
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
oob.err[mtry] = fit$mse[350]
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
oob.err[mtry] = fit$mse[350]
pred = predict(fit, boston[-train,])
test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
#Borramos los datos
rm(list=ls())
#-------------- Pregunta 1: Lectura de datos ----------#
library(MASS)
data(package="MASS")
boston<-Boston
dim(boston)
names(boston)
#-------------- Pregunta 2: datos de train ----------#
set.seed(101)
train = sample(1:nrow(boston), 300) #seleccionamos 300 valores para entrenar
#-------------- Pregunta 3: Ajuste del modelo ----------#
rf.boston = randomForest(medv~., data = boston, subset = train)
rf.boston
#-------------- Pregunta 4: Arboles vs. error ----------#
plot(rf.boston)
rf.boston
summary(rf.boston)
rf.boston
rf.boston$mse
rf.boston$oob.times
randomForest()
randomForest
?randomForest
rf.boston$type
rf.boston$predicted
rf.boston$importance
head(rf.boston$importance)
head(rf.boston$importance[,1])
rf.boston$importance[,2]
#-------------- Pregunta 3: Ajuste del modelo ----------#
rf.boston = randomForest(medv~., data = boston, subset = train,importance=T)
rf.boston
head(rf.boston$importance)
rf.boston$importanceSD
rf.boston$y
rf.boston$inbag
rf.boston$forest
rf.boston$forest$ndbigtree
rf.boston$forest$ntree
rf.boston$forest$xlevels
rf.boston$forest$leftDaughter
nrow(rf.boston$forest$leftDaughter)
221/500
rf.boston$rsq
rf.boston$coefs
rf.boston$oob.times
len(rf.boston$oob.times)
length(rf.boston$oob.times)
length(rf.boston$oob.times)
rf.boston = randomForest(medv~., data = boston, subset = train,ntree=100)
rf.boston
length(rf.boston$oob.times)
rf.boston$oob.times
300/1''
300/100
300/500
print(rf.boston)
ncol(data)
ncol(boston)
ncol(boston)/3
plot(rf.boston)
rf.boston = randomForest(medv~., data = boston, subset = train,ntree=500)
rf.boston
plot(rf.boston)
rf.boston$rsq
mean(rf.boston$rsq)
mean(rf.boston$rsq)*100
rf.boston
rf.boston$rsq
sum(rf.boston$rsq)*1/(500-4)
mean(rf.boston$rsq)
rf.boston
mean(rf.boston$rsq)
sum(rf.boston$rsq)*1/(500-4)
sum(rf.boston$rsq)*1/(500-100)
sum(rf.boston$rsq)*1/(500-4+1)
sum(rf.boston$rsq)*1/(500-5)
sum(rf.boston$rsq)*1/(500-4)
sum(rf.boston$rsq)*1/(500-3)
sum(rf.boston$rsq)*1/(500-2)
var(rf.boston$rsq)
predicted=rf.boston$predicted
y=boston$medv
1 - sum((y-predicted)^2)/sum((y-mean(y))^2)
y
predicted=rf.boston$predicted
y=boston$medv[train]
1 - sum((y-predicted)^2)/sum((y-mean(y))^2)
rf.boston
plot(rf.boston)
#vamos a intentar para cada varlo de mtry posible
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
oob.err[mtry] = fit$mse[350]
pred = predict(fit, boston[-train,])
test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
rf.boston
rf.boston$oob.times
300*0.6
#vamos a intentar para cada varlo de mtry posible
oob.err = double(13)
test.err = double(13)
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
fit$mse
fit$mse
plot(fit$mse)
#vamos a intentar para cada valor de mtry posible
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
oob.err[mtry] = fit$mse[350] #por que elijo aqui 350?
pred = predict(fit, boston[-train,])
test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
