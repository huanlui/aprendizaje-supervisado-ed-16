# Random ForestMúltiples Decision Trees Promediados. Promediar modelos, permite eliminar el overfitting. * En Random Forests, la idea es descorrelacionar los diversos árboles que son generados y luego simplemente reducimos la Varianza en los Decision Trees promediándolos.* Promediar los Decision Trees nos ayuda a reducir la variación y también a mejorar el rendimiento de los árboles de decisión en el conjunto de test y, finalmente, evitar el overfitting.* La idea es construir muchos árboles de tal manera que la correlación entre los árboles sea más pequeña.* Otra cuestión importante acerca de los Random Forest es que podemos seguir agregando más y más árboles grandes y tupidos, y eso no nos perjudicará porque al final solo vamos a promediarlos, lo que reducirá la varianza. ![asdsa](https://bookdown.org/content/2031/images/bootstraping.png)Cada árbol usa un número distinto de variables . Parámetros:- Número de árboles a usar. - Número de variables a escoger aleatoriamente en cada árbol (mtry en la librería). Por defecto coge:   ( número de variables - 1) / 3*Out of bag error* = the mean prediction error on each training sample xᵢ, using only the trees that did not have xᵢ in their bootstrap sample.[1]. Error medio para cada muestra usando sólo los árboles que no contenían dicha muestra en su bag. *Test error*= error con los datos que RF no ha tocado. Vamos a *Random_Forest\Housing_prices_estimation*, al notebook, no a la solución# Support Vector Machines (ver también desde 57 de la presentación)Ver notebook, no solución. Modelos de aprendizaje supervisado. Intenta separar un conjunto de datos mediante un hyperplano.Al vector formado por los puntos más cercanos al hiperplano se le llama vector de soporte.Hay varias soluciones![imagen](https://cdn-images-1.medium.com/max/1200/0*9jEWNXTAao7phK-5.png)Pero intentamos obtener el que tiene máximas separaciones:![imagen](https://cdn-images-1.medium.com/max/1200/0*0o8xIA4k3gXUDCFU.png)Ver [este enlace de Medium](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)¿Pero que pasa para datos no lineales?![no lineal](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1532721364/Fig7_ynn6uj.png)Creamos una tercera dimensión , X^2 + Y^2. Eso es el Kernel Trick.Ventajas:- Alta dimensionalidad. (funciona bien cuando tenemos muchas varaibles). - Poco consumo de memoria: El modelo al final no ocupa mucho , porque son vectores.  - Versátil:podemos separar cosas no separadas linealmene.Desventajas:- Muy sensibles ala elección de parámetros, como el kernel trick. - Sólo te separa por clases, no te da probabilidades. Definición: un conjunto es separable si los puntos quedan de la banda de deciión:![separable](https://mlalgorithm.files.wordpress.com/2016/06/svm1.jpg?w=700)# KNN (K-Nearest Neighbors)Somos lo que nos rodea. Aquí si estamos rodeados de gente de un tipo, se supone que soy de ese tipo. Cogemos los K vecinos más cercanos y será del tipo mayoritario entre esos vecinos. Si los datos están muy remezclados, será mejor que el de Support Vector MAchines. ![knn](https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/03faee64-e85e-4ea0-a2b4-e5964949e2d1/d99b9a4d-618c-45f0-86d1-388bdf852c1d/images/screenshot.gif)# Basic Deep LearningVamos a hacer un clasificador de imágenes. Usamos redes neuronales. Ests redes son potences cuando tenemos muchos tosDe una imagen de 64x 64,, tenemos valores de los tres componentes RGBPor tanto, cada imagne, tenemos 64x64x3 variables +otra que me dice si es un gato o un perro. Ver diapositivas. 