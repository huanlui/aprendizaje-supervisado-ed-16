# Random ForestMúltiples Decision Trees Promediados. Promediar modelos, permite eliminar el overfitting. * En Random Forests, la idea es descorrelacionar los diversos árboles que son generados y luego simplemente reducimos la Varianza en los Decision Trees promediándolos.* Promediar los Decision Trees nos ayuda a reducir la variación y también a mejorar el rendimiento de los árboles de decisión en el conjunto de test y, finalmente, evitar el overfitting.* La idea es construir muchos árboles de tal manera que la correlación entre los árboles sea más pequeña.* Otra cuestión importante acerca de los Random Forest es que podemos seguir agregando más y más árboles grandes y tupidos, y eso no nos perjudicará porque al final solo vamos a promediarlos, lo que reducirá la varianza. ![asdsa](https://bookdown.org/content/2031/images/bootstraping.png)Cada árbol usa un número distinto de variables . Parámetros:- Número de árboles a usar. - Número de variables a escoger aleatoriamente en cada árbol (mtry en la librería). Por defecto coge:   ( número de variables - 1) / 3*Out of bag error* = the mean prediction error on each training sample xᵢ, using only the trees that did not have xᵢ in their bootstrap sample.[1]. Error medio para cada muestra usando sólo los árboles que no contenían dicha muestra en su bag. *Test error*= error con los datos que RF no ha tocado. Vamos a *Random_Forest\Housing_prices_estimation*, al notebook, no a la solución# Support Vector Machines (ver también desde 57 de la presentación)Modelos de aprendizaje supervisado. Intenta separar un conjunto de datos mediante un hyperplano. Ese hyperplano está representado por un vector soporte. Hay varias soluciones![imagen](https://cdn-images-1.medium.com/max/1200/0*9jEWNXTAao7phK-5.png)Pero intentamos obtener el que tiene máximas separaciones:![imagen](https://cdn-images-1.medium.com/max/1200/0*0o8xIA4k3gXUDCFU.png)Ver [este enlace de Medium](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)¿Pero que pasa para datos no lineales![no lineal](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1532721364/Fig7_ynn6uj.png)Creamos una tercera dimensión , X^2 + Y^2. Eso es el Kernel Trick.Ventajas:- Alta dimensionalidad. (funciona bien cuando tenemos muchas varaibles). - Poco consumo de memoria: El modelo al final no ocupa mucho , porque son vectores.  - Versátil:podemos separar cosas no separadas linealmene.Desventajas:- Muy sensibles ala elección de parámetros, como el kernel trick. - Sólo te separa por clases, no te da probabilidades. 